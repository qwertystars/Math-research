{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Algorithm Comparison\n",
    "\n",
    "This notebook compares different optimization algorithms:\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "- Mini-batch Gradient Descent\n",
    "- Gradient Descent with Momentum\n",
    "- Adam Optimizer\n",
    "\n",
    "## Contents\n",
    "1. Algorithm Overview\n",
    "2. Comparison on Simple Functions\n",
    "3. Comparison on Complex Functions\n",
    "4. Performance Metrics\n",
    "5. When to Use Which Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from gradient_descent import (\n",
    "    gradient_descent,\n",
    "    gradient_descent_with_momentum,\n",
    "    adam_optimizer,\n",
    "    quadratic_function,\n",
    "    quadratic_gradient,\n",
    "    rosenbrock_function,\n",
    "    rosenbrock_gradient,\n",
    "    beale_function,\n",
    "    beale_gradient\n",
    ")\n",
    "from comparative_analysis import (\n",
    "    compare_optimization_algorithms,\n",
    "    compare_gd_variants_on_linear_regression,\n",
    "    analyze_learning_rate_sensitivity\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithm Overview\n",
    "\n",
    "### Batch Gradient Descent\n",
    "```\n",
    "θ = θ - α * ∇J(θ)\n",
    "```\n",
    "Uses all training data for each update.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "```\n",
    "For each sample i:\n",
    "    θ = θ - α * ∇J_i(θ)\n",
    "```\n",
    "Uses one sample at a time.\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "```\n",
    "For each batch B:\n",
    "    θ = θ - α * ∇J_B(θ)\n",
    "```\n",
    "Uses small batches of data.\n",
    "\n",
    "### Gradient Descent with Momentum\n",
    "```\n",
    "v = β * v + α * ∇J(θ)\n",
    "θ = θ - v\n",
    "```\n",
    "Adds momentum to accelerate convergence.\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "```\n",
    "m = β₁ * m + (1-β₁) * ∇J(θ)\n",
    "v = β₂ * v + (1-β₂) * (∇J(θ))²\n",
    "θ = θ - α * m̂ / (√v̂ + ε)\n",
    "```\n",
    "Adaptive learning rates for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison on Quadratic Function\n",
    "\n",
    "Let's compare on f(x,y) = x² + y² (simple convex function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([4.0, 4.0])\n",
    "\n",
    "results, paths, costs, times = compare_optimization_algorithms(\n",
    "    f=quadratic_function,\n",
    "    grad_f=quadratic_gradient,\n",
    "    x0=x0,\n",
    "    alpha=0.1,\n",
    "    max_iter=100,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    title=\"Algorithm Comparison on Quadratic Function\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison on Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function is more challenging with a banana-shaped valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "results, paths, costs, times = compare_optimization_algorithms(\n",
    "    f=rosenbrock_function,\n",
    "    grad_f=rosenbrock_gradient,\n",
    "    x0=x0,\n",
    "    alpha=0.001,\n",
    "    max_iter=500,\n",
    "    x_range=(-2, 2),\n",
    "    y_range=(-1, 3),\n",
    "    title=\"Algorithm Comparison on Rosenbrock Function\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison on Beale Function\n",
    "\n",
    "Another challenging test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "results, paths, costs, times = compare_optimization_algorithms(\n",
    "    f=beale_function,\n",
    "    grad_f=beale_gradient,\n",
    "    x0=x0,\n",
    "    alpha=0.001,\n",
    "    max_iter=500,\n",
    "    x_range=(-4, 4),\n",
    "    y_range=(-4, 4),\n",
    "    title=\"Algorithm Comparison on Beale Function\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Regression Comparison\n",
    "\n",
    "Compare Batch, SGD, and Mini-batch on a real ML task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, times = compare_gd_variants_on_linear_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=1,\n",
    "    noise=20.0,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=50,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Sensitivity\n",
    "\n",
    "How does learning rate affect different algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "results = analyze_learning_rate_sensitivity(\n",
    "    f=quadratic_function,\n",
    "    grad_f=quadratic_gradient,\n",
    "    x0=np.array([4.0, 4.0]),\n",
    "    learning_rates=learning_rates,\n",
    "    max_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detailed comparison on multiple functions\n",
    "functions = [\n",
    "    ('Quadratic', quadratic_function, quadratic_gradient, np.array([4.0, 4.0]), 0.1),\n",
    "    ('Rosenbrock', rosenbrock_function, rosenbrock_gradient, np.array([0.0, 0.0]), 0.001),\n",
    "]\n",
    "\n",
    "algorithms = {\n",
    "    'Batch GD': lambda f, g, x, a: gradient_descent(f, g, x, alpha=a, max_iter=500, tol=1e-10),\n",
    "    'Momentum': lambda f, g, x, a: gradient_descent_with_momentum(f, g, x, alpha=a, beta=0.9, max_iter=500, tol=1e-10),\n",
    "    'Adam': lambda f, g, x, a: adam_optimizer(f, g, x, alpha=a*0.1, max_iter=500, tol=1e-10)\n",
    "}\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Function':<15} {'Algorithm':<12} {'Iterations':<12} {'Final Cost':<15} {'Time (s)':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for func_name, f, grad_f, x0, alpha in functions:\n",
    "    for algo_name, algo in algorithms.items():\n",
    "        start_time = time.time()\n",
    "        x_opt, path, cost_history = algo(f, grad_f, x0, alpha)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"{func_name:<15} {algo_name:<12} {len(path)-1:<12} {f(x_opt):<15.6e} {elapsed:<10.4f}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "### When to Use Each Algorithm:\n",
    "\n",
    "**Batch Gradient Descent:**\n",
    "- Small to medium datasets\n",
    "- When you want stable, predictable convergence\n",
    "- When computational cost per iteration is acceptable\n",
    "\n",
    "**Stochastic Gradient Descent:**\n",
    "- Large datasets where batch GD is too slow\n",
    "- When you want to escape local minima\n",
    "- Online learning scenarios\n",
    "\n",
    "**Mini-batch Gradient Descent:**\n",
    "- Most practical real-world applications\n",
    "- Good balance between speed and stability\n",
    "- Works well with modern hardware (GPUs)\n",
    "\n",
    "**Momentum:**\n",
    "- When dealing with noisy gradients\n",
    "- Functions with ravines or valleys\n",
    "- To accelerate convergence\n",
    "\n",
    "**Adam:**\n",
    "- Default choice for many deep learning tasks\n",
    "- Adaptive learning rates work well in practice\n",
    "- Good for problems with sparse gradients\n",
    "- Robust to hyperparameter choices\n",
    "\n",
    "### Performance Characteristics:\n",
    "\n",
    "| Algorithm | Convergence Speed | Stability | Memory | Best Use Case |\n",
    "|-----------|------------------|-----------|--------|---------------|\n",
    "| Batch GD | Slow | High | High | Small datasets |\n",
    "| SGD | Fast | Low | Low | Large datasets |\n",
    "| Mini-batch | Medium | Medium | Medium | General purpose |\n",
    "| Momentum | Fast | Medium | Medium | Noisy objectives |\n",
    "| Adam | Fast | High | Medium | Deep learning |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **No single best algorithm** - depends on problem and constraints\n",
    "2. **Learning rate is critical** - often needs tuning for each problem\n",
    "3. **Momentum helps** in almost all scenarios\n",
    "4. **Adam is robust** but may not always achieve best final performance\n",
    "5. **Mini-batch is practical** for most real-world applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
