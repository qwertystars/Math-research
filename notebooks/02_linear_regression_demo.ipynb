{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Gradient Descent\n",
    "\n",
    "This notebook demonstrates how gradient descent is used to train linear regression models.\n",
    "\n",
    "## Contents\n",
    "1. Linear Regression Theory\n",
    "2. Implementation from Scratch\n",
    "3. Training on Synthetic Data\n",
    "4. Comparison: Batch vs SGD vs Mini-batch\n",
    "5. Real Data Application\n",
    "6. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from linear_regression import (\n",
    "    LinearRegressionGD,\n",
    "    generate_linear_data,\n",
    "    plot_regression_line,\n",
    "    plot_cost_convergence,\n",
    "    compare_gd_methods\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression Theory\n",
    "\n",
    "**Model:** \n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n = \\theta^T x$$\n",
    "\n",
    "**Cost Function (Mean Squared Error):**\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Gradient:**\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "**Update Rule:**\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic linear data\n",
    "X, y, true_theta = generate_linear_data(\n",
    "    n_samples=100,\n",
    "    n_features=1,\n",
    "    noise=10.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(X)} samples\")\n",
    "print(f\"True parameters (intercept, slope): {true_theta}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, s=50)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Generated Linear Data with Noise', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = LinearRegressionGD(\n",
    "    learning_rate=0.1,\n",
    "    max_iter=100,\n",
    "    method='batch'\n",
    ")\n",
    "\n",
    "model.fit(X, y, verbose=True)\n",
    "\n",
    "print(f\"\\nLearned parameters: {model.theta}\")\n",
    "print(f\"True parameters: {true_theta}\")\n",
    "print(f\"\\nR² score: {model.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression line\n",
    "plot_regression_line(X, y, model, title=\"Linear Regression Fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.cost_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Cost (MSE)', fontsize=12)\n",
    "plt.title('Cost Function Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial cost: {model.cost_history[0]:.4f}\")\n",
    "print(f\"Final cost: {model.cost_history[-1]:.4f}\")\n",
    "print(f\"Cost reduction: {(1 - model.cost_history[-1]/model.cost_history[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison: Batch vs SGD vs Mini-batch GD\n",
    "\n",
    "Three variants of gradient descent:\n",
    "\n",
    "1. **Batch Gradient Descent**: Uses all training samples for each update\n",
    "   - Pros: Stable convergence, smooth path\n",
    "   - Cons: Slow for large datasets\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**: Uses one sample at a time\n",
    "   - Pros: Fast updates, can escape local minima\n",
    "   - Cons: Noisy convergence, may oscillate\n",
    "\n",
    "3. **Mini-batch Gradient Descent**: Uses small batches of samples\n",
    "   - Pros: Balance between speed and stability\n",
    "   - Cons: Need to tune batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate larger dataset for comparison\n",
    "X_large, y_large, _ = generate_linear_data(\n",
    "    n_samples=1000,\n",
    "    n_features=1,\n",
    "    noise=15.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(X_large)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the three methods\n",
    "models = compare_gd_methods(\n",
    "    X_large, \n",
    "    y_large,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Learned parameters: {model.theta}\")\n",
    "    print(f\"  R² score: {model.score(X_large, y_large):.4f}\")\n",
    "    print(f\"  Final cost: {model.cost_history[-1]:.4f}\")\n",
    "    print(f\"  Iterations: {len(model.cost_history)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "models_lr = {}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(learning_rates)))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    model = LinearRegressionGD(learning_rate=lr, max_iter=50, method='batch')\n",
    "    model.fit(X, y, verbose=False)\n",
    "    models_lr[f'α={lr}'] = model\n",
    "    \n",
    "    # Plot cost history\n",
    "    ax1.plot(model.cost_history, color=color, linewidth=2, \n",
    "            label=f'α={lr}', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Iteration', fontsize=11)\n",
    "ax1.set_ylabel('Cost (MSE)', fontsize=11)\n",
    "ax1.set_title('Cost Convergence with Different Learning Rates', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot R² scores\n",
    "names = list(models_lr.keys())\n",
    "scores = [model.score(X, y) for model in models_lr.values()]\n",
    "bars = ax2.bar(names, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax2.set_ylabel('R² Score', fontsize=11)\n",
    "ax2.set_title('Model Performance', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Demo\n",
    "\n",
    "Experiment with different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_linear_regression(n_samples=100, noise=10.0, learning_rate=0.1, max_iter=50):\n",
    "    \"\"\"Interactive linear regression demo\"\"\"\n",
    "    \n",
    "    # Generate data\n",
    "    X, y, true_theta = generate_linear_data(\n",
    "        n_samples=n_samples,\n",
    "        n_features=1,\n",
    "        noise=noise,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegressionGD(\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=max_iter,\n",
    "        method='batch'\n",
    "    )\n",
    "    model.fit(X, y, verbose=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left plot: Data and regression line\n",
    "    ax1.scatter(X, y, alpha=0.5, s=50, label='Data')\n",
    "    \n",
    "    X_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "    y_pred = model.predict(X_line)\n",
    "    ax1.plot(X_line, y_pred, 'r-', linewidth=3, label='Regression line')\n",
    "    \n",
    "    # Add equation\n",
    "    equation = f'y = {model.theta[0]:.2f} + {model.theta[1]:.2f}x'\n",
    "    ax1.text(0.05, 0.95, equation, transform=ax1.transAxes,\n",
    "            fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    r2 = model.score(X, y)\n",
    "    ax1.text(0.05, 0.85, f'R² = {r2:.4f}', transform=ax1.transAxes,\n",
    "            fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    ax1.set_xlabel('X', fontsize=12)\n",
    "    ax1.set_ylabel('y', fontsize=12)\n",
    "    ax1.set_title('Linear Regression Fit', fontsize=13, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right plot: Cost history\n",
    "    ax2.plot(model.cost_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.set_xlabel('Iteration', fontsize=12)\n",
    "    ax2.set_ylabel('Cost (MSE)', fontsize=12)\n",
    "    ax2.set_title('Cost Convergence', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f'Samples: {n_samples}\\n'\n",
    "    stats_text += f'Noise: {noise:.1f}\\n'\n",
    "    stats_text += f'Iterations: {len(model.cost_history)-1}\\n'\n",
    "    stats_text += f'Final cost: {model.cost_history[-1]:.2f}'\n",
    "    \n",
    "    ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "            fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    interactive_linear_regression,\n",
    "    n_samples=IntSlider(min=50, max=500, step=50, value=100, description='Samples:'),\n",
    "    noise=FloatSlider(min=1, max=30, step=1, value=10, description='Noise:'),\n",
    "    learning_rate=FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='Learning Rate:'),\n",
    "    max_iter=IntSlider(min=10, max=100, step=10, value=50, description='Max Iterations:')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Linear regression can be solved using gradient descent**\n",
    "2. **The cost function (MSE) is convex** - guaranteed to find global minimum\n",
    "3. **Three variants**: Batch (stable), SGD (fast), Mini-batch (balanced)\n",
    "4. **Learning rate affects convergence speed** - needs tuning\n",
    "5. **R² score measures goodness of fit** (1.0 = perfect, 0.0 = baseline)\n",
    "6. **More data generally leads to better models** (with appropriate parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
