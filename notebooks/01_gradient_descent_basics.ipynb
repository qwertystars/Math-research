{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Basics\n",
    "\n",
    "This notebook demonstrates the fundamentals of gradient descent optimization.\n",
    "\n",
    "## Contents\n",
    "1. Understanding Gradient Descent\n",
    "2. Simple Example: f(x,y) = x² + y²\n",
    "3. Visualizing the Optimization Path\n",
    "4. Effect of Learning Rate\n",
    "5. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from gradient_descent import (\n",
    "    gradient_descent,\n",
    "    quadratic_function,\n",
    "    quadratic_gradient,\n",
    "    rosenbrock_function,\n",
    "    rosenbrock_gradient\n",
    ")\n",
    "from visualizations import (\n",
    "    plot_contour_with_path,\n",
    "    plot_3d_surface_with_path,\n",
    "    plot_gradient_vectors,\n",
    "    plot_learning_rate_comparison\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm for finding the minimum of a function.\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Initialize: x₀ (starting point)\n",
    "Repeat until convergence:\n",
    "    1. Compute gradient: ∇f(xₜ)\n",
    "    2. Update: xₜ₊₁ = xₜ - α * ∇f(xₜ)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- α (alpha) is the learning rate\n",
    "- ∇f(xₜ) is the gradient at point xₜ\n",
    "- The negative sign indicates we move in the opposite direction of the gradient (downhill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Example: f(x,y) = x² + y²\n",
    "\n",
    "Let's start with a simple quadratic function that has a global minimum at (0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define starting point\n",
    "x0 = np.array([4.0, 3.0])\n",
    "\n",
    "# Run gradient descent\n",
    "x_opt, path, cost_history = gradient_descent(\n",
    "    f=quadratic_function,\n",
    "    grad_f=quadratic_gradient,\n",
    "    x0=x0,\n",
    "    alpha=0.1,\n",
    "    max_iter=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting point: {x0}\")\n",
    "print(f\"Optimal point: {x_opt}\")\n",
    "print(f\"Final cost: {quadratic_function(x_opt):.8f}\")\n",
    "print(f\"Number of iterations: {len(path) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Optimization Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on contour plot\n",
    "plot_contour_with_path(\n",
    "    quadratic_function,\n",
    "    path,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    title=\"Gradient Descent on f(x,y) = x² + y²\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize on 3D surface\n",
    "plot_3d_surface_with_path(\n",
    "    quadratic_function,\n",
    "    path,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    title=\"Gradient Descent on 3D Surface\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient vectors\n",
    "plot_gradient_vectors(\n",
    "    quadratic_function,\n",
    "    quadratic_gradient,\n",
    "    x_range=(-5, 5),\n",
    "    y_range=(-5, 5),\n",
    "    grid_density=15,\n",
    "    title=\"Gradient Vector Field (Descent Directions)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost vs iteration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cost_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Cost f(x,y)', fontsize=12)\n",
    "plt.title('Cost Function Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect of Learning Rate\n",
    "\n",
    "The learning rate (α) is crucial:\n",
    "- **Too small**: Slow convergence\n",
    "- **Too large**: Oscillation or divergence\n",
    "- **Just right**: Fast and stable convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "plot_learning_rate_comparison(\n",
    "    f=quadratic_function,\n",
    "    grad_f=quadratic_gradient,\n",
    "    x0=np.array([4.0, 3.0]),\n",
    "    learning_rates=learning_rates,\n",
    "    max_iter=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Demo\n",
    "\n",
    "Experiment with different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_gradient_descent(start_x=4.0, start_y=3.0, alpha=0.1, max_iter=50):\n",
    "    \"\"\"Interactive gradient descent visualization\"\"\"\n",
    "    x0 = np.array([start_x, start_y])\n",
    "    \n",
    "    # Run gradient descent\n",
    "    x_opt, path, cost_history = gradient_descent(\n",
    "        quadratic_function,\n",
    "        quadratic_gradient,\n",
    "        x0,\n",
    "        alpha=alpha,\n",
    "        max_iter=max_iter,\n",
    "        tol=1e-10\n",
    "    )\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left plot: Contour with path\n",
    "    x = np.linspace(-6, 6, 200)\n",
    "    y = np.linspace(-6, 6, 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = X**2 + Y**2\n",
    "    \n",
    "    ax1.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.6)\n",
    "    ax1.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "    \n",
    "    path_array = np.array(path)\n",
    "    ax1.plot(path_array[:, 0], path_array[:, 1], 'r.-', linewidth=2, markersize=8)\n",
    "    ax1.plot(path_array[0, 0], path_array[0, 1], 'go', markersize=15, label='Start')\n",
    "    ax1.plot(path_array[-1, 0], path_array[-1, 1], 'r*', markersize=20, label='End')\n",
    "    \n",
    "    ax1.set_xlabel('x', fontsize=12)\n",
    "    ax1.set_ylabel('y', fontsize=12)\n",
    "    ax1.set_title(f'Optimization Path ({len(path)-1} iterations)', fontsize=13, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Right plot: Cost history\n",
    "    ax2.plot(cost_history, 'b-', linewidth=2, marker='o', markersize=5)\n",
    "    ax2.set_xlabel('Iteration', fontsize=12)\n",
    "    ax2.set_ylabel('Cost', fontsize=12)\n",
    "    ax2.set_title('Cost Convergence', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f'Final point: ({x_opt[0]:.4f}, {x_opt[1]:.4f})\\n'\n",
    "    stats_text += f'Final cost: {quadratic_function(x_opt):.6f}\\n'\n",
    "    stats_text += f'Iterations: {len(path)-1}'\n",
    "    \n",
    "    ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
    "            fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(\n",
    "    interactive_gradient_descent,\n",
    "    start_x=FloatSlider(min=-5, max=5, step=0.5, value=4.0, description='Start X:'),\n",
    "    start_y=FloatSlider(min=-5, max=5, step=0.5, value=3.0, description='Start Y:'),\n",
    "    alpha=FloatSlider(min=0.01, max=0.5, step=0.01, value=0.1, description='Learning Rate:'),\n",
    "    max_iter=IntSlider(min=10, max=100, step=5, value=50, description='Max Iterations:')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function is a more challenging optimization problem with a banana-shaped valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock function: f(x,y) = (1-x)² + 100(y-x²)²\n",
    "# Global minimum at (1, 1)\n",
    "\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "x_opt, path, cost_history = gradient_descent(\n",
    "    rosenbrock_function,\n",
    "    rosenbrock_gradient,\n",
    "    x0,\n",
    "    alpha=0.001,\n",
    "    max_iter=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal point: {x_opt}\")\n",
    "print(f\"True minimum: [1.0, 1.0]\")\n",
    "print(f\"Final cost: {rosenbrock_function(x_opt):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Rosenbrock optimization\n",
    "plot_contour_with_path(\n",
    "    rosenbrock_function,\n",
    "    path,\n",
    "    x_range=(-2, 2),\n",
    "    y_range=(-1, 3),\n",
    "    title=\"Gradient Descent on Rosenbrock Function\",\n",
    "    levels=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradient descent moves in the direction of steepest descent**\n",
    "2. **Learning rate is critical** - needs to be tuned for each problem\n",
    "3. **Different functions have different convergence properties**\n",
    "4. **Visualization helps understand the optimization process**\n",
    "5. **Convergence speed depends on function landscape and parameters**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
